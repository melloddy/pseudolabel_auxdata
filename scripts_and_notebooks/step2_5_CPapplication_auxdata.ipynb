{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import json \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def prob_ncm(scores, labels):\n",
    "    \"\"\"\n",
    "    Converts Neural Network scores into Nonconformity Measures for CP.\n",
    "    Assumes that scores are directly related to the probability of being active\n",
    "    \"\"\"\n",
    "    return np.where( labels > 0, -scores, scores )\n",
    "\n",
    "### p-Values calculation\n",
    "def p_values(calibration_alphas, test_alphas, randomized=False):\n",
    "    sorted_cal_alphas = sorted(calibration_alphas)\n",
    "    if randomized:\n",
    "        # for each test alpha, tieBreaker is the (number of calibration alphas with the same value)*(uniform RV between 0 and 1)\n",
    "        tie_counts = np.searchsorted(sorted_cal_alphas,test_alphas,side='right')-np.searchsorted(sorted_cal_alphas,test_alphas)\n",
    "        tie_breaker = np.random.uniform(size=len(np.atleast_1d(test_alphas)))*tie_counts\n",
    "        return  (len(calibration_alphas)-(np.searchsorted(sorted_cal_alphas,test_alphas,side='right')-tie_breaker)+1)/(len(calibration_alphas)+1)\n",
    "    else:\n",
    "        return  (len(calibration_alphas)-np.searchsorted(sorted_cal_alphas,test_alphas)+1)/(len(calibration_alphas)+1)\n",
    "\n",
    "# Mondrian Inductive Conformal Predictor\n",
    "def micp(calibration_alphas,calibration_labels,test_alphas_0,test_alphas_1,randomized=False):\n",
    "    \"\"\"\n",
    "    Mondrian Inductive Conformal Predictor\n",
    "    Parameters:\n",
    "    calibration_alphas: 1d array of Nonconformity Measures for the calibration examples\n",
    "    calibration_labels: 1d array of labels for the calibration examples - ideally 0/1 or -1/+1,\n",
    "                        but negative/positive values also accepted\n",
    "    test_alpha_0: 1d array of NCMs for the test examples, assuming 0 as label\n",
    "    test_alpha_1: 1d array of NCMs for the test examples, assuming 1 as label\n",
    "    Returns:\n",
    "    p0,p1 : pair of arrays containing the p-values for label 0 and label 1\n",
    "    \"\"\"\n",
    "    if not len(calibration_labels)==len(calibration_alphas):\n",
    "        raise ValueError(\"calibration_labels and calibration alphas must have the same size\")\n",
    "    \n",
    "    if not len(np.atleast_1d(test_alphas_0))==len(np.atleast_1d(test_alphas_1)):\n",
    "        raise ValueError(\"test_alphas_0 and test_alphas_1 must have the same size\")\n",
    "    \n",
    "    p_0 = p_values(calibration_alphas[calibration_labels<=0],\n",
    "                   test_alphas_0,\n",
    "                   randomized)\n",
    "    p_1 = p_values(calibration_alphas[calibration_labels>0],\n",
    "                   test_alphas_1,\n",
    "                   randomized)\n",
    "    return p_0,p_1\n",
    "\n",
    "# function to predict label from p0 and p1\n",
    "def cp_label_predictor(p0, p1, eps):\n",
    "    # Active: p1 > ϵ and p0 ≤ ϵ\n",
    "    # Inactive: p0 > ϵ and p1 ≤ ϵ\n",
    "    # Uncertain (Both): p1 > ϵ and p0 > ϵ\n",
    "    # Empty (None): p1 ≤ ϵ and p0 ≤ ϵ\n",
    "    if p1 > eps and p0 <= eps:\n",
    "        return 1\n",
    "    elif p0 > eps and p1 <= eps:\n",
    "        return 0\n",
    "    elif p0 > eps and p1 > eps:\n",
    "        return 'uncertain both'\n",
    "    elif p0 <= eps and p1 <= eps:\n",
    "        # return 'empty'\n",
    "        # it should actually return 'empty', but to avoid a confusion for people\n",
    "        return 'uncertain none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG \n",
    "\n",
    "eps = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_preds = '../image_predictions/all_cmpds/preds/pred_cpmodel_step2_inference_allcmpds-class.npy'\n",
    "preds = np.load(path_preds,allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30408, 284)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../analysis/cp/labels_fva_dict.json') as fp:\n",
    "    labels_fva_dict = json.load(fp)\n",
    "with open('../analysis/cp/ncms_fva_fit_dict.json') as fp:\n",
    "    ncms_fva_fit_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(np.unique(preds.nonzero()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "indxs = []\n",
    "n_active_preds = []\n",
    "n_inactive_preds = []\n",
    "n_uncertain_preds = []\n",
    "cp_values = {}\n",
    "\n",
    "for col in tqdm(cols):\n",
    "    ncms_fva_col = np.array(ncms_fva_fit_dict[str(col)])\n",
    "    labels_fva_col = np.array(labels_fva_dict[str(col)])\n",
    "    \n",
    "    preds_all_col = preds[:,col].data\n",
    "    \n",
    "    ncms_all_0 = prob_ncm(preds_all_col, np.repeat(0.,len(preds_all_col)))\n",
    "    ncms_all_1 = prob_ncm(preds_all_col, np.repeat(1.,len(preds_all_col)))\n",
    "    \n",
    "    p0, p1 = micp(ncms_fva_col,labels_fva_col,ncms_all_0,ncms_all_1,randomized=False)\n",
    "    cp_all = [cp_label_predictor(pe0, pe1, eps) for pe0, pe1 in zip(p0,p1)]\n",
    "\n",
    "    cp_values[col] = cp_all\n",
    "    \n",
    "    indxs.append(col)\n",
    "    n_active_preds.append(np.array([e==1 for e in cp_values[col]]).sum())\n",
    "    n_inactive_preds.append(np.array([e==0 for e in cp_values[col]]).sum())\n",
    "    n_uncertain_preds.append(np.array([e=='uncertain both' for e in cp_values[col]]).sum())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.DataFrame({\n",
    "    'n_active_pred':n_active_preds\n",
    "    ,'n_inactive_pred':n_inactive_preds\n",
    "    ,'n_uncertain_pred':n_uncertain_preds\n",
    "    ,'col_indx':indxs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_active_pred</th>\n",
       "      <th>n_inactive_pred</th>\n",
       "      <th>n_uncertain_pred</th>\n",
       "      <th>col_indx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [n_active_pred, n_inactive_pred, n_uncertain_pred, col_indx]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_for_aux = df_stats.query('n_active_pred>0 and n_inactive_pred>0')['col_indx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-d6c949ca8107>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-d6c949ca8107>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    ,'standard_qualifier':'='\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "arrs = []\n",
    "for task in tqdm(tasks_for_aux): \n",
    "    arrs.append(pd.DataFrame({\n",
    "            'standard_value':cp_values[task]\n",
    "            ,'input_compound_id': # put an collection with the compound ids here. These don't necessarily need to match the MELLODDY ids, we will map in step2_7 (prev) compound_map['compound_id']\n",
    "            ,'standard_qualifier':'='\n",
    "            ,'input_assay_id':task\n",
    "        }).query('standard_value == 0 or standard_value == 1'))\n",
    "\n",
    "arr = pd.concat(arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./files/image_pseudolabel_aux_nolabels', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.to_csv('./files/image_pseudolabel_aux_nolabels/T1_image_pseudolabel_aux_nolabels.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mellody Tuner",
   "language": "python",
   "name": "melloddy_tuner_regression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
