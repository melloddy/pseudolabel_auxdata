{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_ncm(scores, labels):\n",
    "    \"\"\"\n",
    "    Converts Neural Network scores into Nonconformity Measures for CP.\n",
    "    Assumes that scores are directly related to the probability of being active\n",
    "    \"\"\"\n",
    "    return np.where( labels > 0, -scores, scores )\n",
    "\n",
    "### p-Values calculation\n",
    "def p_values(calibration_alphas, test_alphas, randomized=False):\n",
    "    sorted_cal_alphas = sorted(calibration_alphas)\n",
    "    if randomized:\n",
    "        # for each test alpha, tieBreaker is the (number of calibration alphas with the same value)*(uniform RV between 0 and 1)\n",
    "        tie_counts = np.searchsorted(sorted_cal_alphas,test_alphas,side='right')-np.searchsorted(sorted_cal_alphas,test_alphas)\n",
    "        tie_breaker = np.random.uniform(size=len(np.atleast_1d(test_alphas)))*tie_counts\n",
    "        return  (len(calibration_alphas)-(np.searchsorted(sorted_cal_alphas,test_alphas,side='right')-tie_breaker)+1)/(len(calibration_alphas)+1)\n",
    "    else:\n",
    "        return  (len(calibration_alphas)-np.searchsorted(sorted_cal_alphas,test_alphas)+1)/(len(calibration_alphas)+1)\n",
    "\n",
    "# Mondrian Inductive Conformal Predictor\n",
    "def micp(calibration_alphas,calibration_labels,test_alphas_0,test_alphas_1,randomized=False):\n",
    "    \"\"\"\n",
    "    Mondrian Inductive Conformal Predictor\n",
    "    Parameters:\n",
    "    calibration_alphas: 1d array of Nonconformity Measures for the calibration examples\n",
    "    calibration_labels: 1d array of labels for the calibration examples - ideally 0/1 or -1/+1,\n",
    "                        but negative/positive values also accepted\n",
    "    test_alpha_0: 1d array of NCMs for the test examples, assuming 0 as label\n",
    "    test_alpha_1: 1d array of NCMs for the test examples, assuming 1 as label\n",
    "    Returns:\n",
    "    p0,p1 : pair of arrays containing the p-values for label 0 and label 1\n",
    "    \"\"\"\n",
    "    if not len(calibration_labels)==len(calibration_alphas):\n",
    "        raise ValueError(\"calibration_labels and calibration alphas must have the same size\")\n",
    "    \n",
    "    if not len(np.atleast_1d(test_alphas_0))==len(np.atleast_1d(test_alphas_1)):\n",
    "        raise ValueError(\"test_alphas_0 and test_alphas_1 must have the same size\")\n",
    "    \n",
    "    p_0 = p_values(calibration_alphas[calibration_labels<=0],\n",
    "                   test_alphas_0,\n",
    "                   randomized)\n",
    "    p_1 = p_values(calibration_alphas[calibration_labels>0],\n",
    "                   test_alphas_1,\n",
    "                   randomized)\n",
    "    return p_0,p_1\n",
    "\n",
    "# function to predict label from p0 and p1\n",
    "def cp_label_predictor(p0, p1, eps):\n",
    "    # Active: p1 > ϵ and p0 ≤ ϵ\n",
    "    # Inactive: p0 > ϵ and p1 ≤ ϵ\n",
    "    # Uncertain (Both): p1 > ϵ and p0 > ϵ\n",
    "    # Empty (None): p1 ≤ ϵ and p0 ≤ ϵ\n",
    "    if p1 > eps and p0 <= eps:\n",
    "        return 1\n",
    "    elif p0 > eps and p1 <= eps:\n",
    "        return 0\n",
    "    elif p0 > eps and p1 > eps:\n",
    "        return 'uncertain both'\n",
    "    elif p0 <= eps and p1 <= eps:\n",
    "        # return 'empty'\n",
    "        # it should actually return 'empty', but to avoid a confusion for people\n",
    "        return 'uncertain none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG \n",
    "\n",
    "eps = 0.05\n",
    "fold_va = 2\n",
    "    \n",
    "fva_preds = '../predictions/preds/pred_cpmodel_step1_main_tasks_fold2-class.npy'\n",
    "path_folds = '../data/y2_ext_with_image/matrices/cls/cls_T11_fold_vector.npy'\n",
    "path_labels = '../data/y2_ext_with_image/matrices/cls/cls_T10_y.npy'\n",
    "path_sn = '../data/y2_ext_with_image/results_tmp/folding/T2_folds.csv'\n",
    "path_t5 = '../data/y2_ext_with_image/mapping_table/T5.csv' \n",
    "path_t6_cont = '../data/y2_ext_with_image/results/T10c_cont.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = np.load(path_folds,allow_pickle=True)\n",
    "labels = np.load(path_labels,allow_pickle=True).item()\n",
    "preds_fva = np.load(fva_preds,allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = pd.read_csv(path_sn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_fold2 = sn.query('fold_id == 2')\n",
    "sn_scaffolds = sn_fold2.groupby(by='sn_smiles').count()['input_compound_id'].sort_values(ascending=False)\n",
    "sn_scaffolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_map = sn_scaffolds.reset_index().drop(columns='input_compound_id')\n",
    "sn_map['fold_split'] = np.tile([0,1],reps=len(sn_scaffolds)//2) # ensuring similar size of both groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_mgd = pd.merge(\n",
    "    sn_fold2\n",
    "    ,sn_map\n",
    "    ,how='inner'\n",
    "    ,on='sn_smiles'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sn_mgd) == len(sn_fold2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(sn_mgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link to the cdi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5 \n",
    "input_compound_id,descriptor_vector_id\n",
    "# t6_cont\n",
    "descriptor_vector_id,cont_descriptor_vector_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = pd.read_csv(path_t5)\n",
    "t6_cont = pd.read_csv(path_t6_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgd = pd.merge(\n",
    "        pd.merge(\n",
    "            t5\n",
    "            ,t6_cont\n",
    "            ,how='inner'\n",
    "            ,on='descriptor_vector_id'\n",
    "        ), \n",
    "        sn_mgd\n",
    "        ,how='inner'\n",
    "        ,on='input_compound_id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half of fold2 will be used to fit the CP, \n",
    "# half of fold2 will be used to evaluate the CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_cdvi = pd.DataFrame(\n",
    "    sorted(t6_cont['cont_descriptor_vector_id'].drop_duplicates())\n",
    ")[0].to_dict()# .reset_index()\n",
    "real_cdvi = {v:k for k,v in real_cdvi.items()}\n",
    "real_cdvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgd['real_cont_descriptor_vector_id'] = df_mgd['cont_descriptor_vector_id'].map(real_cdvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdvi_fit = np.array(list(set(df_mgd.query('fold_split == 0')['real_cont_descriptor_vector_id'])))\n",
    "cdvi_eval = np.array(list(set(df_mgd.query('fold_split == 1')['real_cont_descriptor_vector_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdvi_fit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdvi_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CP stuff ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/834 [00:00<?, ?it/s]/app/conda/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:70: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/app/conda/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:73: RuntimeWarning: invalid value encountered in long_scalars\n",
      " 63%|██████▎   | 525/834 [00:24<00:14, 21.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "division by zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 834/834 [00:38<00:00, 21.87it/s]\n"
     ]
    }
   ],
   "source": [
    "e_inacts = []\n",
    "e_acts = []\n",
    "val_inacts = []\n",
    "val_acts = []\n",
    "lit_val_inacts = []\n",
    "lit_val_acts = []\n",
    "unis = []\n",
    "idxs = []\n",
    "n_acts = []\n",
    "n_inacts = []\n",
    "ncms_fva_fit_dict = {}\n",
    "labels_fva_fit_dict = {}\n",
    "\n",
    "\n",
    "for col in tqdm(list(np.unique(preds_fva.nonzero()[1]))):\n",
    "    try: \n",
    "        row_idx_preds_fit = np.intersect1d(\n",
    "            preds_fva[:,col].nonzero()[0]\n",
    "            ,cdvi_fit\n",
    "        )\n",
    "        row_idx_preds_eval = np.intersect1d(\n",
    "            preds_fva[:,col].nonzero()[0]\n",
    "            ,cdvi_eval\n",
    "        )\n",
    "        preds_fva_col = preds_fva[row_idx_preds_fit,col].toarray().squeeze()\n",
    "        preds_fte_col = preds_fva[row_idx_preds_eval,col].toarray().squeeze()\n",
    "        \n",
    "        row_idx_labels_fit = np.intersect1d(\n",
    "            labels[:,col].nonzero()[0]\n",
    "            ,cdvi_fit\n",
    "        )\n",
    "        row_idx_labels_eval = np.intersect1d(\n",
    "            labels[:,col].nonzero()[0]\n",
    "            ,cdvi_eval\n",
    "        )\n",
    "        \n",
    "        labels_fva_col = labels[row_idx_labels_fit,col].toarray().squeeze()\n",
    "        labels_fva_col = np.where(labels_fva_col == -1,0,1)\n",
    "        labels_fte_col = labels[row_idx_labels_eval,col].toarray().squeeze()\n",
    "        labels_fte_col = np.where(labels_fte_col == -1,0,1)\n",
    "\n",
    "        ncms_fva = prob_ncm(preds_fva_col, labels_fva_col)\n",
    "        ncms_fva_fit_dict[str(col)] = ncms_fva.tolist()  # use tolist() to avoid difficulties with the serialisation\n",
    "        labels_fva_fit_dict[str(col)] = labels_fva_col.tolist() # use tolist() to avoid difficulties with the serialisation\n",
    "        #ncms_test_0 = prob_ncm(preds_fte_col, labels_fte_col)\n",
    "        #ncms_test_1 = prob_ncm(preds_fte_col, labels_fte_col)\n",
    "        ncms_test_0 = prob_ncm(preds_fte_col, np.repeat(0.,len(preds_fte_col)))\n",
    "        ncms_test_1 = prob_ncm(preds_fte_col, np.repeat(1.,len(preds_fte_col)))\n",
    "\n",
    "        p0, p1 = micp(ncms_fva,labels_fva_col,ncms_test_0,ncms_test_1,randomized=False)\n",
    "\n",
    "        cp_test = [cp_label_predictor(pe0, pe1, eps) for pe0, pe1 in zip(p0,p1)]\n",
    "        certain_idcs = np.where((np.array(cp_test) == '0') | (np.array(cp_test) == '1'))[0]\n",
    "        idx_uncertain_none = np.where([e == 'uncertain none' for e in cp_test])[0]\n",
    "        idx_uncertain_both = np.where([e == 'uncertain both' for e in cp_test])[0]\n",
    "        idx_inact = np.where(labels_fte_col == 0)[0]\n",
    "        idx_inact_certain = np.intersect1d(idx_inact,certain_idcs)\n",
    "        idx_inact_both = np.intersect1d(idx_inact,idx_uncertain_both)\n",
    "        idx_act = np.where(labels_fte_col == 1)[0]\n",
    "        idx_act_certain = np.intersect1d(idx_act,certain_idcs)\n",
    "        idx_act_both = np.intersect1d(idx_act,idx_uncertain_both)\n",
    "\n",
    "        # efficiency \n",
    "        efficiency_inact = len(idx_inact_certain) / len(idx_inact)\n",
    "        efficiency_act = len(idx_act_certain) / len(idx_act)\n",
    "\n",
    "        # validity \n",
    "        validity_inact = \\\n",
    "             np.sum(np.array(cp_test)[idx_inact_certain] == labels_fte_col[idx_inact_certain].astype(str)) / \\\n",
    "             len(np.array(cp_test)[idx_inact_certain])\n",
    "        validity_act = \\\n",
    "            np.sum(np.array(cp_test)[idx_act_certain] == labels_fte_col[idx_act_certain].astype(str)) / \\\n",
    "            len(np.array(cp_test)[idx_act_certain])\n",
    "\n",
    "        # literature validity \n",
    "        literature_validity_inact = \\\n",
    "             (np.sum(np.array(cp_test)[idx_inact_certain] == labels_fte_col[idx_inact_certain].astype(str)) \\\n",
    "             + len(idx_inact_both)) / \\\n",
    "             len(idx_inact)\n",
    "        literature_validity_act = \\\n",
    "            (np.sum(np.array(cp_test)[idx_act_certain] == labels_fte_col[idx_act_certain].astype(str)) \\\n",
    "            + len(idx_act_both)) / \\\n",
    "            len(idx_act)\n",
    "\n",
    "\n",
    "        uni = np.unique(cp_test)\n",
    "\n",
    "        e_inacts.append(efficiency_inact)\n",
    "        e_acts.append(efficiency_act)\n",
    "        val_inacts.append(validity_inact)\n",
    "        val_acts.append(validity_act)\n",
    "        lit_val_inacts.append(literature_validity_inact)\n",
    "        lit_val_acts.append(literature_validity_act)\n",
    "        unis.append(str(list(uni)))\n",
    "        idxs.append(col)\n",
    "        n_acts.append(len(idx_act))\n",
    "        n_inacts.append(len(idx_inact))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the inputs to the micp() function in order to obtain the CP labels for the inference predictions\n",
    "import json \n",
    "with open('./cp/ncms_fva_fit_dict.json', 'w') as fp:\n",
    "\tjson.dump(ncms_fva_fit_dict, fp)\n",
    "with open('./cp/labels_fva_dict.json', 'w') as fp:\n",
    "\tjson.dump(labels_fva_fit_dict, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'n_inactives_eval':n_inacts\n",
    "    ,'n_actives_eval':n_acts\n",
    "    ,'efficiency_0' : e_inacts\n",
    "    ,'efficiency_1':e_acts\n",
    "    ,'validity_0':val_inacts\n",
    "    ,'validity_1':val_acts\n",
    "    ,'literature_validity_0':lit_val_inacts\n",
    "    ,'literature_validity_1':lit_val_acts\n",
    "    ,'valuess':unis\n",
    "    ,'index':idxs\n",
    "}).to_csv('./cp/summary_eps_' + str(eps) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python36]",
   "language": "python",
   "name": "conda-env-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
