{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, load_npz, save_npz, coo_matrix\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics import roc_auc_score,average_precision_score\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm \n",
    "import os \n",
    "import json \n",
    "import types \n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparsechem import load_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvs = ['0'\n",
    "       ,'0_2'\n",
    "       ,'0_4'\n",
    "       ,'0_5'\n",
    "       ,'0_6'\n",
    "       ,'0_7'\n",
    "       ,'0_8'\n",
    "       ,'0_9'\n",
    "       ,'0_95'\n",
    "       #,'0_99'\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/projects/home/wheyndri/research/wp1_imageauxdata/15_y3_imageaux_study_taskthreshold/notebooks/aux_data/baseline_plus_aux_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels - filtering on the test fold already here in order to drop data\n",
    "fold_te = 0 \n",
    "path = os.path.join(datapath, 'matrices/cls/cls_T11_fold_vector.npy')\n",
    "folds = np.load(path)\n",
    "\n",
    "path = os.path.join(datapath, 'matrices/cls/cls_T10_y.npz')\n",
    "y = load_npz(path)\n",
    "\n",
    "fte_indx = np.where([folds==fold_te])[1]\n",
    "\n",
    "y_df = pd.DataFrame({\n",
    "    'label':coo_matrix(y).data\n",
    "    ,'row':coo_matrix(y).row\n",
    "    ,'col':coo_matrix(y).col\n",
    "}).query('row in @fte_indx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the main tasks that correspond to pseudolabels\n",
    "\n",
    "path = './files/mapping/baseline_image_model_baselineaux_task_mapping.csv'\n",
    "df_matching = pd.read_csv(path)\n",
    "\n",
    "l = list(df_matching['cont_classification_task_id_baseline']) # this is the corresponding main task in the baseline+aux setup\n",
    "y_true_df = y_df.query('col in @l')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ac955d95ded3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# baseline dataset should not contain HTS data : if this is the case, other approach should be taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt8c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt8c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'assay_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'AUX_HTS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cont_classification_task_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my_pseudolabels_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'col in @l'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_df' is not defined"
     ]
    }
   ],
   "source": [
    "# get the lables for the auxiliary tasks that correspond to the pseudolabels\n",
    "\n",
    "path = os.path.join(datapath, 'results_tmp/classification/T8c.csv')\n",
    "t8c = pd.read_csv(path)\n",
    "\n",
    "# baseline dataset should not contain HTS data : if this is the case, other approach should be taken\n",
    "l = list(t8c[t8c['assay_type'] == 'AUX_HTS']['cont_classification_task_id'])\n",
    "y_pseudolabels_df = y_df.query('col in @l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col -> pseudolabel col\n",
    "## strategy : add the cont_classification_task_id via the input_assay_id to the y_pseudolabels_df for merging with the preds\n",
    "\n",
    "df_pseudolabel_to_maintasks = pd.merge(\n",
    "    # connect the aux tasks' cont_classification_task_id from the base+aux model \n",
    "    t8c[['input_assay_id','cont_classification_task_id']] ## this works because every aux task has an unique input assya id \n",
    "    ,df_matching[['cont_classification_task_id_baseline','baseline_compliant_input_assay_id_image', 'input_assay_id_image']]\n",
    "    ,left_on='input_assay_id' ## this works because every aux task has an unique input assya id \n",
    "    ,right_on='baseline_compliant_input_assay_id_image'\n",
    "    ,how='inner'\n",
    "    ,suffixes=('_baselineaux','_matching')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pseudolabels_df_mapped = pd.merge(\n",
    "    y_pseudolabels_df\n",
    "    ,df_pseudolabel_to_maintasks\n",
    "    ,left_on='col'\n",
    "    ,right_on='cont_classification_task_id'\n",
    "    ,how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now start picking up the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../aux_data_predictions/preds_baseline/pred_aux_fold0-class.npy'\n",
    "preds_baseline = np.load(path, allow_pickle=True).item()\n",
    "\n",
    "preds_baseline_df = pd.DataFrame({\n",
    "    'pred':coo_matrix(preds_baseline).data\n",
    "    ,'row':coo_matrix(preds_baseline).row\n",
    "    ,'col':coo_matrix(preds_baseline).col\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching the labels to the predictions for the baseline\n",
    "preds_labels_baseline_df = pd.merge(\n",
    "    preds_baseline_df\n",
    "    ,y_pseudolabels_df_mapped\n",
    "    ,left_on=['row','col']\n",
    "    ,right_on=['row','cont_classification_task_id_baseline']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline + aux data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ppvnpv_df = {}\n",
    "num=1\n",
    "for pv in tqdm(pvs) :\n",
    "    path = f'../aux_data_predictions/ppv_npv_preds/Preds_{str(num).zfill(3)}_ppv{pv}_npv{pv}/preds/pred_aux_fold0-class.npy'\n",
    "    preds_ppvnpv = np.load(path, allow_pickle=True).item()\n",
    "    preds_ppvnpv_df[pv] = pd.DataFrame({\n",
    "        'pred':coo_matrix(preds_ppvnpv).data\n",
    "        ,'row':coo_matrix(preds_ppvnpv).row\n",
    "        ,'col':coo_matrix(preds_ppvnpv).col\n",
    "    })\n",
    "    num+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching the labels to the predictions for the baseline\n",
    "preds_labels_ppvnpv_df = {}\n",
    "\n",
    "for pv in tqdm(pvs) : \n",
    "    preds_labels_ppvnpv_df[pv] = pd.merge(\n",
    "        preds_ppvnpv_df[pv]\n",
    "        ,y_pseudolabels_df_mapped\n",
    "        ,left_on=['row','col']\n",
    "        ,right_on=['row','cont_classification_task_id_baseline']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocs = {}\n",
    "avgprs = {}\n",
    "rocs_baseline = {}\n",
    "avgprs_baseline = {}\n",
    "\n",
    "cols_to_consider = {}\n",
    "rocs_baseline = {}\n",
    "avgprs_baseline = {}\n",
    "\n",
    "for pv in tqdm(pvs) : \n",
    "    \n",
    "    rocs[pv] = []\n",
    "    avgprs[pv] = []\n",
    "    rocs_baseline[pv] = []\n",
    "    avgprs_baseline[pv] = []\n",
    "\n",
    "    cols_to_consider[pv] = preds_labels_ppvnpv_df[pv]['col_x'].drop_duplicates()\n",
    "\n",
    "    for col in cols_to_consider[pv] : \n",
    "        # baseline\n",
    "        arr = preds_labels_baseline_df.query('col_x == @col')\n",
    "        roc = roc_auc_score(arr['label'],arr['pred'])\n",
    "        rocs_baseline[pv].append(roc)\n",
    "        avg_pr = average_precision_score(arr['label'],arr['pred'])\n",
    "        avgprs_baseline[pv].append(avg_pr)\n",
    "        \n",
    "        # with aux data\n",
    "        arr = preds_labels_ppvnpv_df[pv].query('col_x == @col')\n",
    "        roc = roc_auc_score(arr['label'],arr['pred'])\n",
    "        rocs[pv].append(roc)\n",
    "        avg_pr = average_precision_score(arr['label'],arr['pred'])\n",
    "        avgprs[pv].append(avg_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requested plots for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interprete the roc auc as a probability\n",
    "\n",
    "\n",
    "l = [\n",
    "    (np.array(rocs[k])/(1-np.array(rocs[k])))\n",
    "    / \n",
    "    (np.array(rocs_baseline[k])/(1-np.array(rocs_baseline[k])))\n",
    "     for k in rocs.keys()]\n",
    "\n",
    "_ = plt.boxplot(l, showfliers=False) \n",
    "_ = plt.xticks(list(range(1,len(pvs)+1))\n",
    "               ,pvs\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.plot( \n",
    "    [.5,len(pvs)+.5]\n",
    "    ,[1,1]\n",
    "    ,c='blue'\n",
    "    ,ls='--'\n",
    ")\n",
    "plt.ylabel('cross-AUC ROC odds (aux/noaux)')\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection\\(training+evaluation)')\n",
    "plt.grid(ls='--', axis='y')\n",
    "plt.savefig('./results/odds.png'    \n",
    "            ,bbox_inches='tight'\n",
    "            ,pad_inches=0.2\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n"
     ]
    }
   ],
   "source": [
    "agg_tasks = t8c.query('aggregation_weight == 1').query('is_auxiliary == False')['cont_classification_task_id']\n",
    "\n",
    "\n",
    "sizes = [4000]\n",
    "p_roc = []\n",
    "p_pr = []\n",
    "p_spr = []\n",
    "prim_perf = {}\n",
    "prim_perf_baseline = {}\n",
    "\n",
    "path = '/projects/home/wheyndri/research/wp1_imageauxdata/15_y3_imageaux_study_taskthreshold/modelling/models/sc_baseline_plus_aux_baseline_noaux_2010_h4000_ldo0.8_wd1e-06_lr0.001_lrsteps10_ep20_fva0_fte-1.json'\n",
    "#path = datapath + f'/aux_data_training/models/classification_baseline.json'\n",
    "\n",
    "prim_perf_baseline = load_results(path)\n",
    "\n",
    "# s-pr calculation : \n",
    "df_mgd = pd.merge(\n",
    "            prim_perf_baseline['validation']['classification'].loc[agg_tasks]['auc_pr']\n",
    "            ,t8c\n",
    "            ,how='inner'\n",
    "            ,left_index=True\n",
    "            ,right_on='cont_classification_task_id'\n",
    "        )[['auc_pr','num_total_actives','num_total_inactives']]\n",
    "df_mgd['pos_rate'] = df_mgd['num_total_actives']/(df_mgd['num_total_actives']+df_mgd['num_total_inactives'])\n",
    "perf_spr_baseline = np.power(df_mgd['auc_pr']\n",
    "                     ,(np.log(0.5)/np.log(df_mgd['pos_rate']))\n",
    "                    )\n",
    "\n",
    "    \n",
    "    \n",
    "for pv in pvs : \n",
    "    perf_roc = 0 \n",
    "    perf_pr = 0\n",
    "    perf_spr = 0\n",
    "    for size in tqdm(sizes) : \n",
    "        path = '/projects/home/wheyndri/research/wp1_imageauxdata/15_y3_imageaux_study_taskthreshold/modelling/models/sc_baseline_plus_aux_ppv{}_npv{}_2010_h{}_ldo0.8_wd1e-06_lr0.001_lrsteps10_ep20_fva0_fte-1.json'.format(pv,pv,size)\n",
    "        #path = datapath + f'/aux_data_training/ppv_npv_scan/Run_{str(num).zfill(3)}_ppv{pv}_npv{pv}/models/classification_baseline_w_aux.json'\n",
    "\n",
    "        \n",
    "        perf_roc = load_results(path)['validation']['classification'].loc[agg_tasks]['roc_auc_score']        \n",
    "        perf_pr = load_results(path)['validation']['classification'].loc[agg_tasks]['auc_pr']\n",
    "        \n",
    "        # s-pr calculation : \n",
    "        df_mgd = pd.merge(\n",
    "            load_results(path)['validation']['classification'].loc[agg_tasks]['auc_pr']\n",
    "            ,t8c\n",
    "            ,how='inner'\n",
    "            ,left_index=True\n",
    "            ,right_on='cont_classification_task_id'\n",
    "        )[['auc_pr','num_total_actives','num_total_inactives']]\n",
    "        df_mgd['pos_rate'] = df_mgd['num_total_actives']/(df_mgd['num_total_actives']+df_mgd['num_total_inactives'])\n",
    "        perf_spr = np.power(df_mgd['auc_pr']\n",
    "                     ,(np.log(0.5)/np.log(df_mgd['pos_rate']))\n",
    "                    )\n",
    "        \n",
    "    p_roc.append(np.mean(perf_roc))\n",
    "    p_pr.append(np.mean(perf_pr))\n",
    "    p_spr.append(np.mean(perf_spr))\n",
    "p_roc.append(np.mean(prim_perf_baseline['validation']['classification'].loc[agg_tasks]['roc_auc_score']))\n",
    "p_pr.append(np.mean(prim_perf_baseline['validation']['classification'].loc[agg_tasks]['auc_pr']))\n",
    "p_spr.append(np.mean(perf_spr_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(p_spr)\n",
    "         -np.array(p_spr[-1])\n",
    "         , marker='+')\n",
    "plt.ylabel('s-AUC PR')\n",
    "_ = plt.xticks(list(range(len(pvs)+1))\n",
    "               ,pvs+['baseline']\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection\\n(training)')\n",
    "plt.grid(axis='y',ls='--')\n",
    "plt.savefig('./results/global_s_auc_pr.png'\n",
    "           ,bbox_inches='tight'\n",
    "            ,pad_inches=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(p_roc)\n",
    "         -np.array(p_roc[-1])\n",
    "         , marker='+')\n",
    "plt.ylabel('delta AUC ROC')\n",
    "_ = plt.xticks(list(range(len(pvs)+1))\n",
    "               ,pvs+['baseline']\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection\\n(training)')\n",
    "plt.grid(axis='y',ls='--')\n",
    "plt.savefig('./results/global_auc_roc.png'\n",
    "            ,bbox_inches='tight'\n",
    "            ,pad_inches=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks \n",
    "n_non_pseudolabel_tasks = t8c.query('assay_type != \"AUX_HTS\" and cont_classification_task_id == cont_classification_task_id').shape[0]\n",
    "plt.plot(\n",
    "    [len(cols_to_consider[k])/n_non_pseudolabel_tasks for k in rocs.keys()]\n",
    "    ,marker='+'\n",
    ")\n",
    "_ = plt.xticks(list(range(len(pvs)))\n",
    "               ,pvs\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.ylabel('relative amount of pseudolabel tasks')\n",
    "plt.grid(ls='--', axis='y')\n",
    "plt.savefig('./results/relative_tasks.png'\n",
    "           ,bbox_inches='tight'\n",
    "            ,pad_inches=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './analysis/cp/summary_eps_0.05_mgd_cp.csv'\n",
    "df_cp = pd.read_csv(path)\n",
    "\n",
    "cp_t8 = pd.merge(\n",
    "    df_cp\n",
    "    ,t8c\n",
    "    ,how='inner'\n",
    "    ,on='input_assay_id'\n",
    ")\n",
    "\n",
    "t8c.shape\n",
    "\n",
    "arr = t8c.query('cont_classification_task_id == cont_classification_task_id')\n",
    "arr = arr.query('assay_type != \"AUX_HTS\"')\n",
    "nonpseudo_count = np.sum(arr['num_total_actives'] + arr['num_total_inactives'])\n",
    "\n",
    "pvs_num = [0, 0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "\n",
    "ress = []\n",
    "for l1 in pvs_num :\n",
    "    arr = cp_t8.query('validity_0 > @l1').query('validity_1 > @l1')\n",
    "    res = np.sum(arr['num_total_actives'] + arr['num_total_inactives'])\n",
    "    ress.append(res)\n",
    "\n",
    "\n",
    "\n",
    "ress = []\n",
    "for l1 in pvs_num :\n",
    "    arr = cp_t8.query('validity_0 > @l1').query('validity_1 > @l1')\n",
    "    res = np.sum(arr['num_total_actives'] + arr['num_total_inactives'])\n",
    "    ress.append(res)\n",
    "\n",
    "\n",
    "plt.plot(ress / nonpseudo_count\n",
    "         ,marker='+'\n",
    ")\n",
    "_ = plt.xticks(list(range(len(pvs)))\n",
    "               ,pvs\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.ylabel('relative amount of auxiliary datapoints')\n",
    "plt.grid(ls='--', axis='y')\n",
    "plt.savefig('./results/relative_datapoints.png'\n",
    "            ,bbox_inches='tight'\n",
    "            ,pad_inches=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer size -- nice to have if you have done the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [4000,6000,8000]\n",
    "l = []\n",
    "\n",
    "for size in tqdm(sizes) : \n",
    "    \n",
    "    path = '../modelling/models/sc_baseline_plus_aux_baseline_noaux_2010_h4000_ldo0.8_wd1e-06_lr0.001_lrsteps10_ep20_fva0_fte-1.json'\n",
    "    prim_perf_baseline = load_results(path)\n",
    "    prim_perf = {}\n",
    "    \n",
    "    for pv in pvs : \n",
    "        prim_perf[pv] = []\n",
    "        path = '../modelling/models/sc_baseline_plus_aux_ppv{}_npv{}_2010_h{}_ldo0.8_wd1e-06_lr0.001_lrsteps10_ep20_fva0_fte-1.json'.format(pv,pv,size)\n",
    "        prim_perf[pv] = load_results(path)\n",
    "        tmp = pd.DataFrame(\n",
    "            prim_perf[pv]['validation']['classification']['roc_auc_score'][cols_to_consider[pv]]\n",
    "            -prim_perf_baseline['validation']['classification']['roc_auc_score'][cols_to_consider[pv]]\n",
    "        )\n",
    "        tmp['pv'] = pv\n",
    "        tmp['size'] = size\n",
    "        tmp['relative size'] = size/np.min(sizes)\n",
    "        l.append(tmp.reset_index())\n",
    "\n",
    "ccd = pd.concat(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(hue=\"relative size\", y=\"roc_auc_score\",\n",
    "            x=\"pv\",showfliers=False,\n",
    "            data=ccd)  #, palette=[\"m\", \"g\"]\n",
    "plt.plot( \n",
    "    [-.5,len(pvs)-.5]\n",
    "    ,[0,0]\n",
    "    ,c='blue'\n",
    "    ,ls='--'\n",
    ")\n",
    "\n",
    "plt.ylabel('(primary) AUC ROC ')\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.grid(ls='--',axis='y')\n",
    "plt.savefig('./results/layer_size.png'\n",
    "            ,bbox_inches='tight'\n",
    "            ,pad_inches=0.2\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of requested plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interprete the roc auc as a probability\n",
    "\n",
    "\n",
    "l = [\n",
    "    (np.array(rocs[k])/(1-np.array(rocs[k])))\n",
    "    / \n",
    "    (np.array(rocs_baseline[k])/(1-np.array(rocs_baseline[k])))\n",
    "     for k in rocs.keys()]\n",
    "\n",
    "_ = plt.boxplot(l, showfliers=False) \n",
    "_ = plt.xticks(list(range(1,len(pvs)+1))\n",
    "               ,pvs\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.plot( \n",
    "    [.5,len(pvs)+.5]\n",
    "    ,[1,1]\n",
    "    ,c='blue'\n",
    "    ,ls='--'\n",
    ")\n",
    "plt.ylabel('cross-AUC ROC odds (aux/noaux)')\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.grid(ls='--', axis='y')\n",
    "plt.savefig('./results/odds.png'    \n",
    "            ,bbox_inches='tight'\n",
    "            ,pad_inches=0.2\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [np.median(rocs[k]) for k in rocs.keys()]\n",
    "lb = [np.median(rocs_baseline[k]) for k in rocs.keys()]\n",
    "_ = plt.plot(l) \n",
    "_ = plt.plot(lb) \n",
    "_ = plt.xticks(list(range(len(pvs)))\n",
    "               ,pvs\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.grid(axis='y', ls='--')\n",
    "plt.ylabel('median delta cross-AUC ROC')\n",
    "plt.legend(['auxiliary data', 'no auxiliary data'])\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.savefig('./results/cross_auc_roc.png'    \n",
    "            ,bbox_inches='tight'\n",
    "            ,pad_inches=0.2\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [4000,6000,8000]\n",
    "l = []\n",
    "\n",
    "for size in tqdm(sizes) : \n",
    "    \n",
    "    path = '../aux_data_training/models/classification_baseline.json'\n",
    "    prim_perf_baseline = load_results(path)\n",
    "    prim_perf = {}\n",
    "    num=1\n",
    "    for pv in pvs : \n",
    "        prim_perf[pv] = []\n",
    "        path = f'../aux_data_training/ppv_npv_scan/Run_{str(num).zfill(3)}_ppv{pv}_npv{pv}/models/classification_baseline_w_aux.json'\n",
    "        prim_perf[pv] = load_results(path)\n",
    "        tmp = pd.DataFrame(\n",
    "            prim_perf[pv]['validation']['classification']['roc_auc_score'][cols_to_consider[pv]]\n",
    "            -prim_perf_baseline['validation']['classification']['roc_auc_score'][cols_to_consider[pv]]\n",
    "        )\n",
    "        tmp['pv'] = pv\n",
    "        tmp['size'] = size\n",
    "        l.append(tmp.reset_index())\n",
    "        num+=1\n",
    "\n",
    "ccd = pd.concat(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(hue=\"size\", y=\"roc_auc_score\",\n",
    "            x=\"pv\",showfliers=False,\n",
    "            data=ccd)  #, palette=[\"m\", \"g\"]\n",
    "plt.plot( \n",
    "    [-.5,len(pvs)-.5]\n",
    "    ,[0,0]\n",
    "    ,c='blue'\n",
    "    ,ls='--'\n",
    ")\n",
    "\n",
    "plt.ylabel('(primary) AUC ROC ')\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.grid(ls='--',axis='y')\n",
    "plt.savefig('./results/layer_size.png'\n",
    "            ,bbox_inches='tight'\n",
    "            ,pad_inches=0.2\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Primary performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [4000,6000,8000]\n",
    "p_roc = []\n",
    "p_pr = []\n",
    "\n",
    "    \n",
    "num=1  \n",
    "for pv in pvs : \n",
    "    perf_roc = 0 \n",
    "    perf_pr = 0\n",
    "\n",
    "    for size in tqdm(sizes) : \n",
    "        path = f'../aux_data_training/ppv_npv_scan/Run_{str(num).zfill(3)}_ppv{pv}_npv{pv}/models/classification_baseline_w_aux.json'\n",
    "        \n",
    "        perf_roc_new = load_results(path)['validation']['classification_agg']['roc_auc_score']\n",
    "        perf_roc = perf_roc_new if perf_roc<perf_roc_new else perf_roc\n",
    "        \n",
    "        perf_pr_new = load_results(path)['validation']['classification_agg']['auc_pr']\n",
    "        perf_pr = perf_pr_new if perf_pr<perf_pr_new else perf_pr\n",
    "        \n",
    "    p_roc.append(perf_roc)\n",
    "    p_pr.append(perf_pr)\n",
    "    num+=1\n",
    "p_roc.append(prim_perf_baseline['validation']['classification_agg']['roc_auc_score'])\n",
    "p_pr.append(prim_perf_baseline['validation']['classification_agg']['auc_pr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p_roc)\n",
    "plt.ylabel('AUC ROC')\n",
    "_ = plt.xticks(list(range(len(pvs)+1))\n",
    "               ,pvs+['baseline']\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.savefig('./results/global_auc_roc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p_pr)\n",
    "plt.ylabel('AUC PR')\n",
    "_ = plt.xticks(list(range(len(pvs)+1))\n",
    "               ,pvs+['baseline']\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.savefig('./results/global_auc_pr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data volumes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [len(cols_to_consider[k]) for k in rocs.keys() ]\n",
    ")\n",
    "_ = plt.xticks(list(range(len(pvs)))\n",
    "               ,pvs\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.ylabel('number of tasks')\n",
    "plt.grid(ls='--', axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [len(preds_ppvnpv_df[k]) for k in rocs.keys() ]\n",
    "    \n",
    ")\n",
    "_ = plt.xticks(list(range(len(pvs)))\n",
    "               ,pvs\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.ylabel('number of auxiliary datapoints')\n",
    "plt.grid(ls='--', axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(\n",
    "    np.array([len(preds_labels_ppvnpv_df[k]) for k in rocs.keys() ])\n",
    ")\n",
    "_ = plt.xticks(list(range(len(pvs)))\n",
    "               ,pvs\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.ylabel('number of auxiliary datapoints')\n",
    "plt.grid(ls='--', axis='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary vs secondary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    rocs_baseline['0_6']\n",
    "    ,rocs['0_6']\n",
    "    ,alpha=0.2\n",
    ")\n",
    "plt.plot( \n",
    "    [0.3,1]\n",
    "    ,[0.3,1.]\n",
    ")\n",
    "plt.xlabel('baseline')\n",
    "plt.ylabel('with aux data')\n",
    "plt.grid()\n",
    "plt.title('cross-AUC ROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(pvs[:-3]), figsize=(10,55))\n",
    "\n",
    "for i,pv in tqdm(enumerate(pvs[:-3])) : \n",
    "\n",
    "    k = pv\n",
    "    ax[i].scatter(\n",
    "        prim_perf[pv]['validation']['classification']['roc_auc_score'][cols_to_consider[k]]\n",
    "        -prim_perf_baseline['validation']['classification']['roc_auc_score'][cols_to_consider[k]]\n",
    "        , \n",
    "        (np.array(rocs[k])-np.array(rocs_baseline[k]))/np.array(rocs[k])\n",
    "        ,alpha=.3\n",
    "    )\n",
    "    ax[i].grid('--')\n",
    "    ax[i].set_xlabel('delta primary AUC ROC')\n",
    "    ax[i].set_ylim([-.05,.25])\n",
    "    ax[i].set_xlim([-.06,.03])\n",
    "    ax[i].set_ylabel('relative delta cross-AUC ROC (%)')\n",
    "    ax[i].set_title('NPV/PPV threshold {}'.format(pv))\n",
    "    ax[i].plot( \n",
    "    [0,0]\n",
    "    ,[-.05,0.25]\n",
    "    ,c='red'\n",
    "    ,ls='--'\n",
    ")\n",
    "    ax[i].plot( \n",
    "    [-.06,0.03]\n",
    "    ,[0,0]\n",
    "    ,c='red'\n",
    "    ,ls='--'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-melloddy_pipeline]",
   "language": "python",
   "name": "conda-env-.conda-melloddy_pipeline-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
