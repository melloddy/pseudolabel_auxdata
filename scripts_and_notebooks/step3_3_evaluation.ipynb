{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, load_npz, save_npz, coo_matrix\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics import roc_auc_score,average_precision_score\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm \n",
    "import os \n",
    "import json \n",
    "import types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvs = ['0'\n",
    "       ,'0_2'\n",
    "       ,'0_4'\n",
    "       ,'0_5'\n",
    "       ,'0_6'\n",
    "       ,'0_7'\n",
    "       ,'0_8'\n",
    "       ,'0_9'\n",
    "       ,'0_95'\n",
    "       ,'0_99'\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = './files/baseline_plus_aux_data/pseudolabels_plus_baseline/matrices/cls/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels - filtering on the test fold already here in order to drop data\n",
    "fold_te = 0 \n",
    "path = datapath+'cls_T11_fold_vector.npy'\n",
    "folds = np.load(path)\n",
    "\n",
    "path = datapath+'cls_T10_y.npz'\n",
    "y = load_npz(path)\n",
    "\n",
    "fte_indx = np.where([folds==fold_te])[1]\n",
    "\n",
    "y_df = pd.DataFrame({\n",
    "    'label':coo_matrix(y).data\n",
    "    ,'row':coo_matrix(y).row\n",
    "    ,'col':coo_matrix(y).col\n",
    "}).query('row in @fte_indx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422185, 2963)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './cp/summary_eps_0.05_mgd_cp.csv'\n",
    "df_cp = pd.read_csv(path)\n",
    "df_cp = df_cp[['melloddy_col_idx','cont_classification_task_id']] # 'index',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(df_cp['cont_classification_task_id'])\n",
    "y_pseudolabels_df = y_df.query('col in @l')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. for the models with auxilary data (only diagonal for now) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching pseudolabels with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../predictions/preds/pred_inferencemodel_step3_fold0_baseline_plus_aux_ppv0_npv0_-class.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4bee87145f40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpvs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../predictions/preds/pred_inferencemodel_step3_fold0_baseline_plus_aux_ppv{}_npv{}_-class.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpreds_ppvnpv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     preds_ppvnpv_df = pd.DataFrame({\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m'pred'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_ppvnpv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rama.jabal/.conda/envs/melloddy_tuner_regression/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../predictions/preds/pred_inferencemodel_step3_fold0_baseline_plus_aux_ppv0_npv0_-class.npy'"
     ]
    }
   ],
   "source": [
    "preds_ppvnpv = {}\n",
    "preds_labels_ppvnpv_df = {}\n",
    "for pv in tqdm(pvs) : \n",
    "    path = '../predictions/preds/pred_inferencemodel_step3_fold0_baseline_plus_aux_ppv{}_npv{}_-class.npy'.format(pv,pv)\n",
    "    preds_ppvnpv[pv] = np.load(path, allow_pickle=True).item()\n",
    "    preds_ppvnpv_df = pd.DataFrame({\n",
    "        'pred':coo_matrix(preds_ppvnpv[pv]).data\n",
    "        ,'row':coo_matrix(preds_ppvnpv[pv]).row\n",
    "        ,'col':coo_matrix(preds_ppvnpv[pv]).col\n",
    "    })\n",
    "\n",
    "    preds_ppvnpv_df = pd.merge(\n",
    "        preds_ppvnpv_df\n",
    "        ,df_cp\n",
    "        ,how='inner'\n",
    "        ,right_on='melloddy_col_idx'\n",
    "        ,left_on='col'\n",
    "    )\n",
    "\n",
    "    # merging with the fold 0 predictions will reduce the pseudolabel data to fold 0\n",
    "    preds_labels_ppvnpv_df[pv] = pd.merge(\n",
    "        preds_ppvnpv_df\n",
    "        ,y_pseudolabels_df\n",
    "            # col field from the baseline_plus_aux_data y matrix \n",
    "        ,right_on=['row','col'] \n",
    "            # cont_classification_task_id from cp/summary_eps_0.05_mgd_cp file : ccti for the base+aux dataset in step 3 \n",
    "        ,left_on=['row','cont_classification_task_id'] \n",
    "        ,how='inner'\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    f = 'cls_T10_y_ppv{}_npv{}.npz'.format(pv,pv)\n",
    "    t10 = load_npz(os.path.join(datapath,f))\n",
    "    s = set(t10.nonzero()[1])\n",
    "    preds_labels_ppvnpv_df_tmp = preds_labels_ppvnpv_df[pv].query('cont_classification_task_id in @s')\n",
    "    preds_labels_baseline_df_tmp = preds_labels_baseline_df.query('cont_classification_task_id in @s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rocs = {}\n",
    "avgprs = {}\n",
    "sizes = {}\n",
    "fracs = {}\n",
    "\n",
    "for pv in tqdm(pvs) : \n",
    "    rocs[pv] = []\n",
    "    avgprs[pv] = []\n",
    "    sizes[pv] = []\n",
    "    fracs[pv] = [] \n",
    "    \n",
    "    # limit ourselves to the cols that are actually populated in the training\n",
    "    f = './confidence_selection/cls_T10_y_ppv{}_npv{}.npz'.format(pv,pv)\n",
    "    t10 = load_npz(os.path.join(datapath,f))\n",
    "    s = np.unique(t10.nonzero()[1])\n",
    "    cols_to_consider = preds_labels_ppvnpv_df[pv].query('cont_classification_task_id in @s')['cont_classification_task_id'].drop_duplicates()\n",
    "    \n",
    "    for col in cols_to_consider:\n",
    "        arr = preds_labels_ppvnpv_df[pv].query('cont_classification_task_id == @col')\n",
    "        try:\n",
    "            roc = roc_auc_score(arr['label'],arr['pred'])\n",
    "            avgpr = average_precision_score(arr['label'],arr['pred'])\n",
    "            rocs[pv].append(roc)\n",
    "            avgprs[pv].append(avgpr)\n",
    "            sizes[pv].append(len(arr))\n",
    "            fracs[pv].append(arr['label'].value_counts().loc[-1]/arr.shape[0])\n",
    "    \n",
    "        except: \n",
    "            print(col)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. for the models without auxilary data (baseline) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../predictions/preds/pred_inferencemodel_step3_fold0_baseline-class.npy'\n",
    "preds_baseline = np.load(path, allow_pickle=True).item()\n",
    "\n",
    "preds_baseline_df = pd.DataFrame({\n",
    "    'pred':coo_matrix(preds_baseline).data\n",
    "    ,'row':coo_matrix(preds_baseline).row\n",
    "    ,'col':coo_matrix(preds_baseline).col\n",
    "})\n",
    "\n",
    "preds_baseline_df = pd.merge(\n",
    "    preds_baseline_df\n",
    "    ,df_cp\n",
    "    ,how='inner'\n",
    "    ,right_on='melloddy_col_idx'\n",
    "    ,left_on='col'\n",
    ")\n",
    "\n",
    "preds_labels_baseline_df = pd.merge(\n",
    "    preds_baseline_df\n",
    "    ,y_pseudolabels_df\n",
    "    ,right_on=['row','col']\n",
    "    ,left_on=['row','cont_classification_task_id']\n",
    "    ,how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head-to-head comparison of both models will require equal amount of datapoints \n",
    "assert preds_labels_ppvnpv_df['0_9'].shape == preds_labels_baseline_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts = preds_baseline_df['cont_classification_task_id'].value_counts()\n",
    "# take two random records - values should match for the baseline - dense predictions with no confidence selection\n",
    "assert cnts.iloc[0] == cnts.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts = y_pseudolabels_df['col'].value_counts()\n",
    "# take two random records - values will only match in rare cases (confidence row selection for every task)\n",
    "assert cnts.iloc[0] != cnts.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocs_baseline = {}\n",
    "avgprs_baseline = {}\n",
    "sizes_baseline = {}\n",
    "fracs_baseline = {}\n",
    "\n",
    "\n",
    "for pv in tqdm(pvs) : \n",
    "\n",
    "    rocs_baseline[pv] = []\n",
    "    avgprs_baseline[pv] = []\n",
    "    sizes_baseline[pv] = []\n",
    "    fracs_baseline[pv] = []\n",
    "    \n",
    "    # limit ourselves to the cols that are actually populated in the training\n",
    "    f = './confidence_selection/cls_T10_y_ppv{}_npv{}.npz'.format(pv,pv)\n",
    "    t10 = load_npz(os.path.join(datapath,f))\n",
    "    s = np.unique(t10.nonzero()[1])\n",
    "    cols_to_consider = preds_labels_ppvnpv_df[pv].query('cont_classification_task_id in @s')['cont_classification_task_id'].drop_duplicates()\n",
    "    \n",
    "    for col in cols_to_consider:\n",
    "        arr = preds_labels_baseline_df.query('cont_classification_task_id == @col')\n",
    "        try:\n",
    "            roc = roc_auc_score(arr['label'],arr['pred'])\n",
    "            avgpr = average_precision_score(arr['label'],arr['pred'])\n",
    "            rocs_baseline[pv].append(roc)\n",
    "            avgprs_baseline[pv].append(avgpr)\n",
    "            sizes_baseline[pv].append(len(arr))\n",
    "            fracs_baseline[pv].append(arr['label'].value_counts().loc[-1]/arr.shape[0])\n",
    "    \n",
    "        except: \n",
    "            print(col)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.median(rocs_baseline['0'])!= np.median(rocs_baseline['0_99'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "\n",
    "for pv in pvs : \n",
    "    diffs.append(np.array(rocs[pv]) - np.array(rocs_baseline[pv]))\n",
    "diffs.append(np.array(rocs_baseline['0']) - np.array(rocs_baseline['0']))\n",
    "_ = plt.boxplot(diffs)\n",
    "plt.grid(ls='--')\n",
    "_ = plt.xticks(\n",
    "    range(1,len(diffs)+1)\n",
    "    ,l[:]\n",
    "    ,rotation=90\n",
    ")\n",
    "plt.ylabel('Delta cross-AUCROC \\n(on set of tasks with \\nan image-based counterpart)')\n",
    "plt.xlabel('NPV and PPV thresholds (diagonal)')\n",
    "_ = plt.plot([0.5,len(pvs)+1+0.5],[0.,0.], linestyle='--', color='blue')\n",
    "_ = plt.ylim([-.25,.25])\n",
    "plt.savefig(\n",
    "    './figures/delta_cross_AUC_diff.svg'\n",
    "    ,bbox_inches='tight'\n",
    "    ,pad_inches=0\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary activity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/projects/home/wheyndri/git/performance_evaluation/development/')\n",
    "import modeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_results(filename, two_heads=False):\n",
    "    \"\"\"Loads conf and results from a file\n",
    "    Args:\n",
    "        filename    name of the json/npy file\n",
    "        two_heads   set up class_output_size if missing\n",
    "    \"\"\"\n",
    "    if filename.endswith(\".npy\"):\n",
    "        return np.load(filename, allow_pickle=True).item()\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for key in [\"model_type\"]:\n",
    "        if key not in data[\"conf\"]:\n",
    "            data[\"conf\"][key] = None\n",
    "    if two_heads and (\"class_output_size\" not in data[\"conf\"]):\n",
    "        data[\"conf\"][\"class_output_size\"] = data[\"conf\"][\"output_size\"]\n",
    "        data[\"conf\"][\"regr_output_size\"]  = 0\n",
    "\n",
    "    data[\"conf\"] = types.SimpleNamespace(**data[\"conf\"])\n",
    "\n",
    "\n",
    "    if \"results\" in data:\n",
    "        for key in data[\"results\"]:\n",
    "            data[\"results\"][key] = pd.read_json(data[\"results\"][key])\n",
    "\n",
    "    if \"results_agg\" in data:\n",
    "        for key in data[\"results_agg\"]:\n",
    "            data[\"results_agg\"][key] = pd.read_json(data[\"results_agg\"][key], typ=\"series\")\n",
    "\n",
    "    for key in [\"training\", \"validation\"]:\n",
    "        if key not in data:\n",
    "            continue\n",
    "        for dfkey in [\"classification\", \"regression\"]:\n",
    "            data[key][dfkey] = pd.read_json(data[key][dfkey])\n",
    "        for skey in [\"classification_agg\", \"regression_agg\"]:\n",
    "            data[key][skey]  = pd.read_json(data[key][skey], typ=\"series\")\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../modelling/baseline_plus_aux/models/sc_baseline_2010_h4000_ldo0.8_wd1e-06_lr0.001_lrsteps10_ep20_fva0_fte-1.json'\n",
    "prim_perf_baseline = load_results(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../modelling/baseline_plus_aux/models/sc_baseline_plus_aux_ppv0_6_npv0_6_2010_h4000_ldo0.8_wd1e-06_lr0.001_lrsteps10_ep20_fva0_fte-1.json'\n",
    "prim_perf_0_6 = load_results(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_labels_baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = '0_6'\n",
    "f = './confidence_selection/cls_T10_y_ppv{}_npv{}.npz'.format(pv,pv)\n",
    "t10 = load_npz(os.path.join(datapath,f))\n",
    "s = np.unique(t10.nonzero()[1])\n",
    "cols_to_consider = preds_labels_ppvnpv_df[pv].query('cont_classification_task_id in @s')['cont_classification_task_id'].drop_duplicates()\n",
    "main_tasks = df_cp.query('cont_classification_task_id in @cols_to_consider')['melloddy_col_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'inferno'\n",
    "cmap = plt.get_cmap(name)\n",
    "perfcolors = cmap(prim_perf['validation']['classification'].loc[main_tasks].sort_index()['roc_auc_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    prim_perf_0_6['validation']['classification'].loc[main_tasks].sort_index()['roc_auc_score'] - prim_perf_baseline['validation']['classification'].loc[main_tasks].sort_index()['roc_auc_score']\n",
    "    ,np.array(rocs[pv])-np.array(rocs_baseline[pv])\n",
    "    ,alpha=.3\n",
    "    ,c=perfcolors\n",
    "    ,s=np.log10(prim_perf_baseline['validation']['classification'].loc[main_tasks].sort_index()['num_pos'])*3\n",
    ")\n",
    "plt.grid('--')\n",
    "plt.xlabel('delta primary AUC ROC')\n",
    "plt.ylabel('delta cross-AUC ROC')\n",
    "_ = plt.title('NPV/PPV threshold {}'.format(pv))\n",
    "\n",
    "plt.savefig(\n",
    "    './figures/delta_cross_AUC_vs_primary.svg'\n",
    "    ,bbox_inches='tight'\n",
    "    ,pad_inches=0\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct colorbar \n",
    "mn = prim_perf['validation']['classification'].loc[main_tasks].sort_index()['roc_auc_score'].min()\n",
    "mx = prim_perf['validation']['classification'].loc[main_tasks].sort_index()['roc_auc_score'].max()\n",
    "im = plt.imshow([[mn,mn],[mx,mx]],cmap)\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primarys = []\n",
    "secondarys = []\n",
    "for pv in tqdm(pvs) : \n",
    "    \n",
    "    path = '../modelling/baseline_plus_aux/models/sc_baseline_plus_aux_ppv{}_npv{}_2010_h4000_ldo0.8_wd1e-06_lr0.001_lrsteps10_ep20_fva0_fte-1.json'.format(pv,pv)\n",
    "    prim_perf = load_results(path)\n",
    "    \n",
    "    f = './confidence_selection/cls_T10_y_ppv{}_npv{}.npz'.format(pv,pv)\n",
    "    t10 = load_npz(os.path.join(datapath,f))\n",
    "    s = np.unique(t10.nonzero()[1])\n",
    "    cols_to_consider = preds_labels_ppvnpv_df[pv].query('cont_classification_task_id in @s')['cont_classification_task_id'].drop_duplicates()\n",
    "    main_tasks = df_cp.query('cont_classification_task_id in @cols_to_consider')['melloddy_col_idx']\n",
    "    secondary = np.median(np.array(rocs[pv])-np.array(rocs_baseline[pv]))\n",
    "    secondarys.append(np.median(secondary))\n",
    "    primary = np.nanmedian(prim_perf['validation']['classification'].loc[main_tasks].sort_index()['roc_auc_score'] - prim_perf_baseline['validation']['classification'].loc[main_tasks].sort_index()['roc_auc_score'])\n",
    "    primarys.append(np.median(primary))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(primarys)\n",
    "_ = plt.plot(secondarys)\n",
    "_ = plt.xticks(\n",
    "    range(len(primarys))\n",
    "    ,l[:-1]\n",
    "    ,rotation=90\n",
    ")\n",
    "_ = plt.legend([\n",
    "    'primary'\n",
    "    ,'secondary'\n",
    "    \n",
    "])\n",
    "plt.savefig(\n",
    "    './figures/primary_vs_secondary.svg'\n",
    "    ,bbox_inches='tight'\n",
    "    ,pad_inches=0\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mellody Tuner",
   "language": "python",
   "name": "melloddy_tuner_regression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
