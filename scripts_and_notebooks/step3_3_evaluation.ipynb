{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, load_npz, save_npz, coo_matrix\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics import roc_auc_score,average_precision_score\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm \n",
    "import os \n",
    "import json \n",
    "import types \n",
    "\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparsechem import load_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvs = ['0'\n",
    "       ,'0_2'\n",
    "       ,'0_4'\n",
    "       ,'0_5'\n",
    "       ,'0_6'\n",
    "       ,'0_7'\n",
    "       ,'0_8'\n",
    "       ,'0_9'\n",
    "       ,'0_95'\n",
    "       #,'0_99'\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../aux_data_preperation/baseline_plus_aux_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels - filtering on the test fold already here in order to drop data\n",
    "fold_te = 0 \n",
    "path = os.path.join(datapath, 'matrices/cls/cls_T11_fold_vector.npy')\n",
    "folds = np.load(path)\n",
    "\n",
    "path = os.path.join(datapath, 'matrices/cls/cls_T10_y.npz')\n",
    "y = load_npz(path)\n",
    "\n",
    "fte_indx = np.where([folds==fold_te])[1]\n",
    "\n",
    "y_df = pd.DataFrame({\n",
    "    'label':coo_matrix(y).data\n",
    "    ,'row':coo_matrix(y).row\n",
    "    ,'col':coo_matrix(y).col\n",
    "}).query('row in @fte_indx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the main tasks that correspond to pseudolabels\n",
    "\n",
    "path = './files/mapping/baseline_image_model_baselineaux_task_mapping.csv'\n",
    "df_matching = pd.read_csv(path)\n",
    "\n",
    "l = list(df_matching['cont_classification_task_id']) # this is the corresponding main task in the baseline+aux setup\n",
    "y_true_df = y_df.query('col in @l')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the lables for the auxiliary tasks that correspond to the pseudolabels\n",
    "\n",
    "path = os.path.join(datapath, 'results_tmp/classification/T8c.csv')\n",
    "t8c = pd.read_csv(path)\n",
    "\n",
    "# baseline dataset should not contain HTS data : if this is the case, other approach should be taken\n",
    "l = list(t8c[t8c['assay_type'] == 'AUX_HTS']['cont_classification_task_id'])\n",
    "y_pseudolabels_df = y_df.query('col in @l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col -> pseudolabel col\n",
    "## strategy : add the cont_classification_task_id via the input_assay_id to the y_pseudolabels_df for merging with the preds\n",
    "\n",
    "df_pseudolabel_to_maintasks = pd.merge(\n",
    "    # connect the aux tasks' cont_classification_task_id from the base+aux model \n",
    "    t8c[['input_assay_id','cont_classification_task_id']] ## this works because every aux task has an unique input assya id \n",
    "    ,df_matching[['cont_classification_task_id','baseline_compliant_input_assay_id_image']]\n",
    "    ,left_on='input_assay_id' ## this works because every aux task has an unique input assya id \n",
    "    ,right_on='baseline_compliant_input_assay_id_image'\n",
    "    ,how='inner'\n",
    "    ,suffixes=('_baselineaux','_matching')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pseudolabels_df_mapped = pd.merge(\n",
    "    y_pseudolabels_df\n",
    "    ,df_pseudolabel_to_maintasks\n",
    "    ,left_on='col'\n",
    "    ,right_on='cont_classification_task_id_baselineaux'\n",
    "    ,how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now start picking up the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../predictions/preds/pred_inferencemodel_step3_fold0_baseline_plus_aux_baseline_noaux_-class.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-105bc6094df0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../predictions/preds/pred_inferencemodel_step3_fold0_baseline_plus_aux_baseline_noaux_-class.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m preds_baseline_df = pd.DataFrame({\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'pred'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_baseline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rama.jabal/.conda/envs/melloddy_tuner_regression/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../predictions/preds/pred_inferencemodel_step3_fold0_baseline_plus_aux_baseline_noaux_-class.npy'"
     ]
    }
   ],
   "source": [
    "path = '../predictions/preds/pred_inferencemodel_step3_fold0_baseline_plus_aux_baseline_noaux_-class.npy'\n",
    "preds_baseline = np.load(path, allow_pickle=True).item()\n",
    "\n",
    "preds_baseline_df = pd.DataFrame({\n",
    "    'pred':coo_matrix(preds_baseline).data\n",
    "    ,'row':coo_matrix(preds_baseline).row\n",
    "    ,'col':coo_matrix(preds_baseline).col\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds_baseline_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f149ceb0cd21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# matching the labels to the predictions for the baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m preds_labels_baseline_df = pd.merge(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpreds_baseline_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m,\u001b[0m\u001b[0my_pseudolabels_df_mapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m,\u001b[0m\u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'row'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'col'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds_baseline_df' is not defined"
     ]
    }
   ],
   "source": [
    "# matching the labels to the predictions for the baseline\n",
    "preds_labels_baseline_df = pd.merge(\n",
    "    preds_baseline_df\n",
    "    ,y_pseudolabels_df_mapped\n",
    "    ,left_on=['row','col']\n",
    "    ,right_on=['row','cont_classification_task_id_matching']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline + aux data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ppvnpv_df = {}\n",
    "for pv in tqdm(pvs) : \n",
    "    path = '../predictions/preds/pred_inferencemodel_step3_fold0_baseline_plus_aux_ppv{}_npv{}_-class.npy'.format(pv,pv)\n",
    "    preds_ppvnpv = np.load(path, allow_pickle=True).item()\n",
    "    preds_ppvnpv_df[pv] = pd.DataFrame({\n",
    "        'pred':coo_matrix(preds_ppvnpv).data\n",
    "        ,'row':coo_matrix(preds_ppvnpv).row\n",
    "        ,'col':coo_matrix(preds_ppvnpv).col\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching the labels to the predictions for the baseline\n",
    "preds_labels_ppvnpv_df = {}\n",
    "\n",
    "for pv in tqdm(pvs) : \n",
    "    preds_labels_ppvnpv_df[pv] = pd.merge(\n",
    "        preds_ppvnpv_df[pv]\n",
    "        ,y_pseudolabels_df_mapped\n",
    "        ,left_on=['row','col']\n",
    "        ,right_on=['row','cont_classification_task_id_matching']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocs = {}\n",
    "avgprs = {}\n",
    "rocs_baseline = {}\n",
    "avgprs_baseline = {}\n",
    "\n",
    "cols_to_consider = {}\n",
    "rocs_baseline = {}\n",
    "avgprs_baseline = {}\n",
    "\n",
    "for pv in tqdm(pvs) : \n",
    "    \n",
    "    rocs[pv] = []\n",
    "    avgprs[pv] = []\n",
    "    rocs_baseline[pv] = []\n",
    "    avgprs_baseline[pv] = []\n",
    "\n",
    "    cols_to_consider[pv] = preds_labels_ppvnpv_df[pv]['col_x'].drop_duplicates()\n",
    "\n",
    "    for col in cols_to_consider[pv] : \n",
    "        # baseline\n",
    "        arr = preds_labels_baseline_df.query('col_x == @col')\n",
    "        roc = roc_auc_score(arr['label'],arr['pred'])\n",
    "        rocs_baseline[pv].append(roc)\n",
    "        avg_pr = average_precision_score(arr['label'],arr['pred'])\n",
    "        avgprs_baseline[pv].append(avg_pr)\n",
    "        \n",
    "        # with aux data\n",
    "        arr = preds_labels_ppvnpv_df[pv].query('col_x == @col')\n",
    "        roc = roc_auc_score(arr['label'],arr['pred'])\n",
    "        rocs[pv].append(roc)\n",
    "        avg_pr = average_precision_score(arr['label'],arr['pred'])\n",
    "        avgprs[pv].append(avg_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interprete the roc auc as a probability\n",
    "\n",
    "\n",
    "l = [\n",
    "    (np.array(rocs[k])/(1-np.array(rocs[k])))\n",
    "    / \n",
    "    (np.array(rocs_baseline[k])/(1-np.array(rocs_baseline[k])))\n",
    "     for k in rocs.keys()]\n",
    "\n",
    "_ = plt.boxplot(l, showfliers=False) \n",
    "_ = plt.xticks(list(range(1,len(pvs)+1))\n",
    "               ,pvs\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.plot( \n",
    "    [.5,len(pvs)+.5]\n",
    "    ,[1,1]\n",
    "    ,c='blue'\n",
    "    ,ls='--'\n",
    ")\n",
    "plt.ylabel('cross-AUC ROC odds (aux/noaux)')\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.grid(ls='--', axis='y')\n",
    "plt.savefig('./results/odds.png'    \n",
    "            ,bbox_inches='tight'\n",
    "            ,pad_inches=0.2\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [np.median(rocs[k]) for k in rocs.keys()]\n",
    "lb = [np.median(rocs_baseline[k]) for k in rocs.keys()]\n",
    "_ = plt.plot(l) \n",
    "_ = plt.plot(lb) \n",
    "_ = plt.xticks(list(range(len(pvs)))\n",
    "               ,pvs\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.grid(axis='y', ls='--')\n",
    "plt.ylabel('median delta cross-AUC ROC')\n",
    "plt.legend(['auxiliary data', 'no auxiliary data'])\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.savefig('./results/cross_auc_roc.png'    \n",
    "            ,bbox_inches='tight'\n",
    "            ,pad_inches=0.2\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [4000,6000,8000]\n",
    "l = []\n",
    "\n",
    "for size in tqdm(sizes) : \n",
    "    \n",
    "    path = '../modelling/models/sc_baseline_plus_aux_baseline_noaux_2010_h4000_ldo0.8_wd1e-06_lr0.001_lrsteps10_ep20_fva0_fte-1.json'\n",
    "    prim_perf_baseline = load_results(path)\n",
    "    prim_perf = {}\n",
    "    \n",
    "    for pv in pvs : \n",
    "        prim_perf[pv] = []\n",
    "        path = '../modelling/models/sc_baseline_plus_aux_ppv{}_npv{}_2010_h{}_ldo0.8_wd1e-06_lr0.001_lrsteps10_ep20_fva0_fte-1.json'.format(pv,pv,size)\n",
    "        prim_perf[pv] = load_results(path)\n",
    "        tmp = pd.DataFrame(\n",
    "            prim_perf[pv]['validation']['classification']['roc_auc_score'][cols_to_consider[pv]]\n",
    "            -prim_perf_baseline['validation']['classification']['roc_auc_score'][cols_to_consider[pv]]\n",
    "        )\n",
    "        tmp['pv'] = pv\n",
    "        tmp['size'] = size\n",
    "        l.append(tmp.reset_index())\n",
    "\n",
    "ccd = pd.concat(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(hue=\"size\", y=\"roc_auc_score\",\n",
    "            x=\"pv\",showfliers=False,\n",
    "            data=ccd)  #, palette=[\"m\", \"g\"]\n",
    "plt.plot( \n",
    "    [-.5,len(pvs)-.5]\n",
    "    ,[0,0]\n",
    "    ,c='blue'\n",
    "    ,ls='--'\n",
    ")\n",
    "\n",
    "plt.ylabel('(primary) AUC ROC ')\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.grid(ls='--',axis='y')\n",
    "plt.savefig('./results/layer_size.png'\n",
    "            ,bbox_inches='tight'\n",
    "            ,pad_inches=0.2\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Primary performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [4000,6000,8000]\n",
    "p_roc = []\n",
    "p_pr = []\n",
    "\n",
    "    \n",
    "    \n",
    "for pv in pvs : \n",
    "    perf_roc = 0 \n",
    "    perf_pr = 0\n",
    "    for size in tqdm(sizes) : \n",
    "        prim_perf[pv] = []\n",
    "        path = '../modelling/models/sc_baseline_plus_aux_ppv{}_npv{}_2010_h{}_ldo0.8_wd1e-06_lr0.001_lrsteps10_ep20_fva0_fte-1.json'.format(pv,pv,size)\n",
    "        \n",
    "        perf_roc_new = load_results(path)['validation']['classification_agg']['roc_auc_score']\n",
    "        perf_roc = perf_roc_new if perf_roc<perf_roc_new else perf_roc\n",
    "        \n",
    "        perf_pr_new = load_results(path)['validation']['classification_agg']['auc_pr']\n",
    "        perf_pr = perf_pr_new if perf_pr<perf_pr_new else perf_pr\n",
    "        \n",
    "    p_roc.append(perf_roc)\n",
    "    p_pr.append(perf_pr)\n",
    "p_roc.append(prim_perf_baseline['validation']['classification_agg']['roc_auc_score'])\n",
    "p_pr.append(prim_perf_baseline['validation']['classification_agg']['auc_pr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p_roc)\n",
    "plt.ylabel('AUC ROC')\n",
    "_ = plt.xticks(list(range(len(pvs)+1))\n",
    "               ,pvs+['baseline']\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.savefig('./results/global_auc_roc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p_pr)\n",
    "plt.ylabel('AUC PR')\n",
    "_ = plt.xticks(list(range(len(pvs)+1))\n",
    "               ,pvs+['baseline']\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.savefig('./results/global_auc_pr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data volumes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [len(cols_to_consider[k]) for k in rocs.keys() ]\n",
    ")\n",
    "_ = plt.xticks(list(range(len(pvs)))\n",
    "               ,pvs\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.ylabel('number of tasks')\n",
    "plt.grid(ls='--', axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [len(preds_ppvnpv_df[k]) for k in rocs.keys() ]\n",
    "    \n",
    ")\n",
    "_ = plt.xticks(list(range(len(pvs)))\n",
    "               ,pvs\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.ylabel('number of auxiliary datapoints')\n",
    "plt.grid(ls='--', axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(\n",
    "    np.array([len(preds_labels_ppvnpv_df[k]) for k in rocs.keys() ])\n",
    ")\n",
    "_ = plt.xticks(list(range(len(pvs)))\n",
    "               ,pvs\n",
    "               , rotation=90\n",
    "              )\n",
    "plt.xlabel('task quality criterium (NPV and PPV > threshold)\\nused for task selection')\n",
    "plt.ylabel('number of auxiliary datapoints')\n",
    "plt.grid(ls='--', axis='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary vs secondary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    rocs_baseline['0_6']\n",
    "    ,rocs['0_6']\n",
    "    ,alpha=0.2\n",
    ")\n",
    "plt.plot( \n",
    "    [0.3,1]\n",
    "    ,[0.3,1.]\n",
    ")\n",
    "plt.xlabel('baseline')\n",
    "plt.ylabel('with aux data')\n",
    "plt.grid()\n",
    "plt.title('cross-AUC ROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(pvs), figsize=(10,55))\n",
    "\n",
    "for i,pv in tqdm(enumerate(pvs)) : \n",
    "\n",
    "    k = pv\n",
    "    ax[i].scatter(\n",
    "        prim_perf[pv]['validation']['classification']['roc_auc_score'][cols_to_consider[k]]\n",
    "        -prim_perf_baseline['validation']['classification']['roc_auc_score'][cols_to_consider[k]]\n",
    "        , \n",
    "        (np.array(rocs[k])-np.array(rocs_baseline[k]))/np.array(rocs[k])\n",
    "        ,alpha=.3\n",
    "    )\n",
    "    ax[i].grid('--')\n",
    "    ax[i].set_xlabel('delta primary AUC ROC')\n",
    "    ax[i].set_ylim([-.05,.25])\n",
    "    ax[i].set_xlim([-.06,.03])\n",
    "    ax[i].set_ylabel('relative delta cross-AUC ROC (%)')\n",
    "    ax[i].set_title('NPV/PPV threshold {}'.format(pv))\n",
    "    ax[i].plot( \n",
    "    [0,0]\n",
    "    ,[-.05,0.25]\n",
    "    ,c='red'\n",
    "    ,ls='--'\n",
    ")\n",
    "    ax[i].plot( \n",
    "    [-.06,0.03]\n",
    "    ,[0,0]\n",
    "    ,c='red'\n",
    "    ,ls='--'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-melloddy_pipeline]",
   "language": "python",
   "name": "conda-env-.conda-melloddy_pipeline-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
