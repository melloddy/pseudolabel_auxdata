{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, lil_matrix, load_npz, save_npz, hstack\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<422240x2981 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 4882954 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner_output_baseline_w_aux = '../aux_data_preperation/baseline_plus_aux_data/'\n",
    "path = os.path.join(tuner_output_baseline_w_aux, 'matrices/cls/cls_T10_y.npz')\n",
    "y_aux = load_npz(path)\n",
    "y_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sparse_main_fold0 = lil_matrix(y_aux.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the y-sparse matrix for inference \n",
    "# Main tasks + fold 0 to limit the amount of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(tuner_output_baseline_w_aux, 'matrices/cls/cls_T11_fold_vector.npy')\n",
    "folds = np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './files/mapping/baseline_image_model_baselineaux_task_mapping.csv'\n",
    "df_matching = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50   NaN\n",
       "51   NaN\n",
       "52   NaN\n",
       "53   NaN\n",
       "54   NaN\n",
       "Name: cont_classification_task_id, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "50    1710.0\n",
       "51    1898.0\n",
       "52    2941.0\n",
       "53    2942.0\n",
       "54    2944.0\n",
       "Name: cont_classification_task_id_baseline, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# main tasks \n",
    "# mind the difference between these two !\n",
    "# cont_classification_task_id is the baseline + aux model \n",
    "display(df_matching.query('validity_0 >= 0').query('validity_1 >= 0')['cont_classification_task_id'].tail())\n",
    "display(df_matching.query('validity_0 >= 0').query('validity_1 >= 0')['cont_classification_task_id_baseline'].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [0\n",
    "       ,0.2\n",
    "       ,0.4\n",
    "       ,0.5\n",
    "       ,0.6\n",
    "       ,0.7\n",
    "       ,0.8\n",
    "       ,0.9\n",
    "       ,0.95\n",
    "       ,0.99\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 139.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "27\n",
      "16\n",
      "10\n",
      "5\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i,l1 in tqdm(enumerate(lst)):\n",
    "    l2 = l1\n",
    "    print(df_matching.query('validity_0 > @l1').query('validity_1 > @l2')['cont_classification_task_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:06,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# new approach : for inference, only consider the main tasks that correspond to aux tasks meeting the criteria \n",
    "# instead of one general sparse y matrix \n",
    "\n",
    "base_path = '../aux_data_preperation/baseline_plus_aux_data/matrices/cls/fold_0'\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "for i,l1 in tqdm(enumerate(lst)):\n",
    "    y_sparse_main_fold0 = lil_matrix(y_aux.shape)\n",
    "    l2 = l1\n",
    "    #for j,l2 in enumerate(lst): \n",
    "    file_core = 'ppv{}_npv{}'.format(l1,l2).replace('.','_')\n",
    "    col_idxs = set(df_matching.query('validity_0 > @l1').query('validity_1 > @l2')['cont_classification_task_id'].dropna())\n",
    "    for col in col_idxs :\n",
    "        y_sparse_main_fold0[folds == 0,col] = 1\n",
    "    path = os.path.join(base_path, 'y_sparse_main_tasks_fold0_{}'.format(file_core))\n",
    "    save_npz(path,csr_matrix(y_sparse_main_fold0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mellody Tuner",
   "language": "python",
   "name": "melloddy_tuner_regression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
