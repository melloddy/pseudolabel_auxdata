## Installation

### Poetry

[After installing poetry](https://python-poetry.org/docs/#installation),
 you can install the environment:
```bash
poetry install
```

Don't forget to update the lock file if the dependencies have change:
```bash
poetry lock
```

## Pipeline Overview

### 1 - Data preprocessing
- Using *T2_images.csv*, T0, T1, T2 files are generated for image compounds that exist also in Melloddy data with their corresponding tasks.
- The newly generated T files are processed by Melloddy Tuner
- The X file generated by Melloddy Tuner is replaced by image features from *T_images_features.csv*

### 2 - Image-based model training
- A hyperparameter scan is run on the image-based model using images features.
- The best image-based model is selected based on the ROC-AUC score and used for future predictions

### 3 - Fitting Conformal Predictors
- Infer using image-based model on fold 2 of only labeled datapoints of the image dataset
- Fit conformal predictors, half of the fold 2 will be used for fitting, half for evaluation.

### 4 - Generating pseudolabels
- Infer using image-based model on all datapoints of the image dataset, including non-MELLODDY compounds.
- Use outputted predictions to generate pseudolabels via conformal predictors.

### 5 - Generating T files for pseudolabels
- generate T1-style pseudolabel file from predictions
- replace predictions with their true values from main tasks when they exist
- Generate pseudolabel auxiliary data T0,T1,T2 files


## Instructions

### Step 0: Prepare Input

Prepare the following files:
* Melloddy dataset
  - T files of the Melloddy dataset.
  - The folder containing the output of Melloddy-Tuner on the Melloddy dataset.
* Image dataset
  - T2 that contains all compounds with images (make sure that the used *input_compound_id* match their analogs in your MELLODDY data when they exist). it will later be referred to as *T2_images.csv*
  - A file with columns corresponding to image features (standardized) and indexed with the input_compound_id. The file preperation manual is available on box (https://app.box.com/file/852061069648?s=xc2iqr0nylz0p73tj1vfapr2apuuxyx5). In the scripts it's referred to as *T_image_features_std.csv*


### Step 2: Write the configuration file
To run the pipeline a configuration file need to be provided to set hyperparameters and directories.
The file *config.json* is an example configuration file that can be adjusted accordingly.
The list of parameters to be defined are :
* **Data related parameters**
  - `t0_melloddy_path` : the path to the T0 Melloddy file
  - `t1_melloddy_path` : the path to the T1 Melloddy file
  - `t2_melloddy_path` : the path to the T2 Melloddy file
  - `t2_images_path`: the path to T2-like csv file containing all image compounds
  - `t_images_features_path`: the path to the csv file containing image features and indexed with the input_compound_id
  - `tuner_output_folder_baseline` : the path to the folder containing the output of data preparation of Melloddy data using tuner
  - `output_folder_path`: the path to the output folder
* **Melloddy tuner related parameters**
  - `key_json` : the path to json key file for Melloddy Tuner
  -`parameters_json` : the path to json parameters file for Melloddy Tuner
  - `ref_hash_json` : the path to json reference hash file for Melloddy Tuner
* **Sparsechem related parameters**
  - `torch_device`: device to use to run sparsechem CPU or GPU
  - `max_cpu`: maximum number of CPU to use when CPU is selected
  - `dataloader_num_workers` : number of worker to use in dataloader in sparsechem (needs to be set to 0 when using CPU on macOS)
* **Image model hyperparameter** : when not precised the default value of hyperparameters defined in *pseudolabel/constants.py* are used
  - `imagemodel_hidden_size` : list of hidden layers' sizes for image-based model's hyperparameter scan. A list of string for each hidden size layer.
  - `imagemodel_epochs_lr_steps`: list of tuple of number of epochs and corresponding learning rates for image-based model's hyperparameter scan
  - `imagemodel_dropouts`: list of number of dropouts for image-based model's hyperparameter scan
  - `show_progress` : when set true, the progess of the image-based model's hyperparameter scan is shown in the logs

### Step 3: Run pipeline
You can run the pipeline after configuring thr *config.json* file with the following command on the CLI:

```bash
 pseudolabel-pipe -c config.json
```


## Output files
The pipeline saves different files in the output folder specified. The outputted files are:

* **analysis** : Contains stats on auxiliary tasks
* **hyperopt** : Contains image-based models from the hyper-optimisation scan
* **image_model_data** : Contains T files and processed data by Melloddy Tuner for the image dataset
* **intermediate** : Contains intermediate files, used by the pipeline
* **logs** : log files of sparsechem
* **pseudolabels** : final T files containing auxiliary pseudolabels
